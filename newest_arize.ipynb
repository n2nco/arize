{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datetime import timedelta\n",
    "from functools import reduce\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "# from gcsfs import GCSFileSystem\n",
    "from IPython.display import YouTubeVideo\n",
    "from llama_index.graph_stores.simple import SimpleGraphStore\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.callbacks import CallbackManager, OpenInferenceCallbackHandler\n",
    "from llama_index.callbacks.open_inference_callback import as_dataframe\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index import ServiceContext, LLMPredictor\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.response.schema import Response\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import textwrap\n",
    "from typing import List, Union\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleWebPageReader,\n",
    "    ServiceContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.callbacks import CallbackManager, OpenInferenceCallbackHandler\n",
    "from llama_index.callbacks.open_inference_callback import as_dataframe, QueryData\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "openai_api_key = \"oops"\n",
    "assert openai_api_key != \"copy paste your api key here\", \"âŒ Please set your OpenAI API key\"\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xrWnNZvqk78vHGib5E64T3BlbkFJjsQdTlKJucJGD\"\n",
    "print('working dir:', os.getcwd()) \n",
    "from llama_index import VectorStoreIndex, load_index_from_storage\n",
    "from llama_index.callbacks.schema import CBEventType, EventPayload\n",
    "from dataclasses import asdict\n",
    "\n",
    "# ## Create a storage context from our tweets documents\n",
    "\n",
    "# index = load_index_from_storage(service_context=service_context)\n",
    "# service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n",
    "# storage_context = StorageContext.from_defaults(persist_dir='tweets_by_authenticexit/')\n",
    "# index = load_index_from_storage(storage_context=storage_context, service_context=service_context)\n",
    "# query_engine = index.as_query_engine()\n",
    "\n",
    "callback_handler = OpenInferenceCallbackHandler()\n",
    "callback_manager = CallbackManager([callback_handler])\n",
    "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n",
    "storage_context = StorageContext.from_defaults(persist_dir='../tweets_by_authenticexit/')\n",
    "index = load_index_from_storage(storage_context=storage_context, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "# r = index.as_retriever(query='what did he say about hotub? ', num_results=4) \n",
    "# retrieved = r.retrieve('hottub')\n",
    "\n",
    "\n",
    "\n",
    "# r_nodex = query_engine.retrieve(QueryBundle(query_str=\"What did he say about hotub?\"))\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"What did he say about hotub?\",\n",
    "    \"What does he think about primary residences?\",\n",
    "    \"what does he say about interest rates?\",\n",
    "    \"Can he throw down and show down?\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# =========== non manual - calls the callback_manager and callback_handler lifecycle methods\n",
    "# nodes = query_engine.query(\"What did he say about hotub?\")\n",
    "# nodes2 = query_engine.query(\"What does he think about primary residences?\")\n",
    "\n",
    "\n",
    "# callback_manager.event(CBEventType.LLM, payload=EventPayload.RESPONSE: 'test response from llm yhall') as llm_event:\n",
    "#    llm_event.on_end(payload={EventPayload.NODES: nodes})\n",
    "\n",
    "\n",
    "#++================= manual\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    # callback_handler._query_data_buffer.append(callback_handler._trace_data.query_data)\n",
    "    # callback_handler._node_data_buffer.extend(callback_handler._trace_data.node_datas)\n",
    "\n",
    "    query_bundle = QueryBundle(query_str=query)\n",
    "    callback_handler.start_trace(trace_id=\"query\") #pseudo query to call start_trace\n",
    "    # callback_manager.\n",
    "    # Use the callback_manager to manage an event for the retrieval operation\n",
    "    with callback_manager.event(CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}) as retrieve_event:\n",
    "        # callback_manager.start_trace(trace_id=\"query\") #pseudo query to call start_trace\n",
    "        nodes = query_engine.retrieve(query_bundle)\n",
    "        # When retrieval is complete, end the event and pass the retrieved nodes as payload\n",
    "        # retrieve_event.on_end(payload={EventPayload.NODES: nodes})\n",
    "\n",
    "        max_characters_per_line = 80\n",
    "        print(\"Query=====\")\n",
    "        print(textwrap.fill(query, max_characters_per_line ))\n",
    "        print(\"Retrieved Nodes===============\")\n",
    "        print(textwrap.fill(str(nodes), max_characters_per_line))\n",
    "\n",
    "        # retrieve_event.on_end(payload={EventPayload.RESPONSE: 'tesst response'})\n",
    "\n",
    "\n",
    "        callback_handler.on_event_end(event_type=CBEventType.RETRIEVE, payload={EventPayload.NODES: nodes})\n",
    "        callback_handler.on_event_end(event_type=CBEventType.LLM, payload={EventPayload.RESPONSE: 'test response from llm yhall'})\n",
    "        # callback_manager.end_trace(trace_id=\"query\") #pseudo query to call end_trace\n",
    "        callback_handler.end_trace(trace_id=\"query\") #pseudo query to call end_trace\n",
    "\n",
    "\n",
    "        # print()\n",
    "        # result_dict = evaluate(nodes)  # Replace with your evaluation function\n",
    "        # sample_query_df = sample_query_df.append(result_dict, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_dataframe(callback_handler):\n",
    "    query_data_list = callback_handler.flush_query_data_buffer()\n",
    "    node_data_list = callback_handler.flush_node_data_buffer()\n",
    "    \n",
    "    records = []\n",
    "    for query_data in query_data_list:\n",
    "        breakpoint\n",
    "        record = {\n",
    "            \":feature.text:prompt\": query_data.query_text,\n",
    "            \":feature.[float].embedding:prompt\": query_data.query_embedding,\n",
    "            \":feature.[str].retrieved_document_ids:prompt\": query_data.node_ids,\n",
    "\n",
    "            \":prediction.text:response\": query_data.response_text,\n",
    "            \":timestamp.iso_8601:\": query_data.timestamp,\n",
    "            \":feature.[float].retrieved_document_scores:prompt\": query_data.scores\n",
    "        }\n",
    "        for i, node_id in enumerate(query_data.node_ids):\n",
    "            node_data = next((nd for nd in node_data_list if nd.id == node_id), None)\n",
    "            if node_data:\n",
    "                # record[f\"node_id_{i}\"] = node_id\n",
    "                # record[f\"retrieved_document_text_{i}\"] = node_data.node_text\n",
    "                record[f\":tag.str:openai_relevance_{i}\"] = query_data.scores[i]\n",
    "                # record['node_embedding_{i}'] = node_data.node_embedding\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    print('record before df: ', records)\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "df = generate_dataframe(callback_handler)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir='../tweets_by_authenticexit'\n",
    " # pass default graph store to prevent unauthorized request to GCS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storage_context_to_dataframe(storage_context: StorageContext) -> pd.DataFrame:\n",
    "    \"\"\"Converts the storage context to a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        storage_context (StorageContext): Storage context containing the index\n",
    "        data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe containing the index data.\n",
    "    \"\"\"\n",
    "    document_ids = []\n",
    "    document_texts = []\n",
    "    document_embeddings = []\n",
    "    docstore = storage_context.docstore\n",
    "    vector_store = storage_context.vector_store\n",
    "    for node_id, node in docstore.docs.items():\n",
    "        document_ids.append(node.hash)  # use node hash as the document ID\n",
    "        document_texts.append(node.text)\n",
    "        document_embeddings.append(np.array(vector_store.get(node_id)))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"document_id\": document_ids,\n",
    "            \"text\": document_texts,\n",
    "            \"text_vector\": document_embeddings,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "database_df = storage_context_to_dataframe(storage_context)\n",
    "database_df = database_df.drop_duplicates(subset=[\"text\"])\n",
    "# database_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "database_embedding_column_name = \"text_vector\"\n",
    "database_centroid = database_df[database_embedding_column_name].apply(np.mean).mean()\n",
    "database_df[database_embedding_column_name] = database_df[database_embedding_column_name].apply(\n",
    "    lambda x: np.array(x) - database_centroid\n",
    ")\n",
    "\n",
    "query_embedding_column_name = \":feature.[float].embedding:prompt\"\n",
    "query_centroid = df[query_embedding_column_name].apply(np.mean).mean()\n",
    "df[query_embedding_column_name] = df[query_embedding_column_name].apply(\n",
    "    lambda x: np.array(x) - query_centroid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your original DataFrame\n",
    "# Create a list of columns to drop\n",
    "columns_to_drop = [\n",
    "    'retrieved_document_text_0', 'retrieved_document_text_1',\n",
    "    'retrieved_document_text_2', 'openai_relevance_2'\n",
    "]\n",
    "\n",
    "# Drop the specified columns\n",
    "new_df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Drop the duplicate column ':tag.str:openai_relevance_0'\n",
    "# You can use the 'duplicated' method to find duplicates and drop one of them\n",
    "new_df = new_df.loc[:, ~new_df.columns.duplicated()]\n",
    "\n",
    "# Display the first few rows of the new DataFrame\n",
    "new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_schema = px.Schema(\n",
    "    prediction_id_column_name=\"document_id\",\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        vector_column_name=\"text_vector\",\n",
    "        raw_data_column_name=\"text\",\n",
    "    )\n",
    ")\n",
    "database_ds = px.Dataset(\n",
    "    dataframe=database_df,\n",
    "    schema=database_schema,\n",
    "    name=\"database\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns={\n",
    "#     \"openai_relevance_0\": \":tag.str:openai_relevance_0\",\n",
    "#     \"openai_relevance_1\": \":tag.str:openai_relevance_1\"\n",
    "# }, inplace=True)\n",
    "\n",
    "# #insert two needs columns to calc ranking metrics\n",
    "# df[\":tag.float:openai_precision_at_1\"] = None\n",
    "# df[\":tag.float:openai_precision_at_2\"] = None\n",
    "\n",
    "# df.head()\n",
    "\n",
    "num_retrieved_documents = 3\n",
    "num_relevant_documents_array = np.zeros(len(df))\n",
    "\n",
    "# Calculating number of relevant documents for each retrieved document and storing it in precision column.\n",
    "for retrieved_context_index in range(num_retrieved_documents):\n",
    "    num_relevant_documents_array += df[f\":tag.str:openai_relevance_{retrieved_context_index}\"].apply(lambda x: 1 if x == \"relevant\" else 0).values\n",
    "    df[f\":tag.float:openai_precision_at_{retrieved_context_index + 1}\"] = num_relevant_documents_array.tolist() / (retrieved_context_index + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_SYSTEM_MESSAGE = \"You will be given a query and a reference text. You must determine whether the reference text contains an answer to the input query. Your response must be binary (0 or 1) and should not contain any text or characters aside from 0 or 1. 0 means that the reference text does not contain an answer to the query. 1 means the reference text contains an answer to the query.\"\n",
    "QUERY_CONTEXT_PROMPT_TEMPLATE = \"\"\"# Query: {query}\n",
    "\n",
    "# Reference: {reference}\n",
    "\n",
    "# Binary: \"\"\"\n",
    "num_retrieved_documents = 3\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def evaluate_query_and_retrieved_context(query: str, context: str, model_name: str) -> str:\n",
    "    prompt = QUERY_CONTEXT_PROMPT_TEMPLATE.format(\n",
    "        query=query,\n",
    "        reference=context,\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EVALUATION_SYSTEM_MESSAGE},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def evaluate_retrievals(\n",
    "    retrievals_data: Dict[str, str],\n",
    "    model_name: str,\n",
    ") -> List[str]:\n",
    "    responses = []\n",
    "    for query, retrieved_context in tqdm(retrievals_data.items()):\n",
    "        response = evaluate_query_and_retrieved_context(query, retrieved_context, model_name)\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "\n",
    "def process_binary_responses(\n",
    "    binary_responses: List[str], binary_to_string_map: Dict[int, str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse binary responses and convert to the desired format\n",
    "    converts them to the desired format. The binary_to_string_map parameter\n",
    "    should be a dictionary mapping binary values (0 or 1) to the desired\n",
    "    string values (e.g. \"irrelevant\" or \"relevant\").\n",
    "    \"\"\"\n",
    "    processed_responses = []\n",
    "    for binary_response in binary_responses:\n",
    "        try:\n",
    "            binary_value = int(binary_response.strip())\n",
    "            processed_response = binary_to_string_map[binary_value]\n",
    "        except (ValueError, KeyError):\n",
    "            processed_response = None\n",
    "        processed_responses.append(processed_response)\n",
    "    return processed_responses\n",
    "\n",
    "\n",
    "evaluation_model_name = \"gpt-4\"\n",
    "document_id_to_text = dict(zip(database_df[\"document_id\"], database_df[\"text\"]))\n",
    "query_texts = df[\":feature.text:prompt\"].to_list()\n",
    "for retrieved_document_index in range(num_retrieved_documents):\n",
    "    retrieved_document_ids = [\n",
    "        doc_ids[retrieved_document_index]\n",
    "        for doc_ids in df[\":feature.[str].retrieved_document_ids:prompt\"].to_list()\n",
    "    ]\n",
    "    retrieved_document_texts = [document_id_to_text[doc_id] for doc_id in retrieved_document_ids]\n",
    "    retrievals_data = dict(zip(query_texts, retrieved_document_texts))\n",
    "    raw_responses = evaluate_retrievals(retrievals_data, evaluation_model_name)\n",
    "    processed_responses = process_binary_responses(raw_responses, {0: \"irrelevant\", 1: \"relevant\"})\n",
    "    df[\n",
    "        f\"retrieved_document_text_{retrieved_document_index}\"\n",
    "    ] = retrieved_document_texts\n",
    "    df[f\"openai_relevance_{retrieved_document_index}\"] = processed_responses\n",
    "\n",
    "\n",
    "df[\n",
    "    [\n",
    "        \":feature.text:prompt\",\n",
    "        \"retrieved_document_text_0\",\n",
    "        \"retrieved_document_text_1\",\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "    ]\n",
    "].rename(columns={\":feature.text:prompt\": \"prompt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ds = px.Dataset.from_open_inference(new_df)\n",
    "print(query_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session = px.launch_app(primary=query_ds, corpus=database_ds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
